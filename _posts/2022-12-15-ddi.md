---
layout: distill
title: Dissecting Distribution Inference
description:  Describing our work on distribution inference attacks.
tags:
- property inference
- privacy
- distribution inference
date: 2022-12-15
thumbnail: assets/img/ddi/featured.png
citation: false
featured: false
pretty_table: true
categories: paper

authors:
  - name: Anshuman Suri
    url: "https://anshumansuri.com/"
    affiliations:
      name: University of Virginia

bibliography: combined.bib

toc:
  - name: KL Divergence Attack
  - name: Impact of adversaryâ€™s knowledge
  - name: Defenses
  - name: Summary
---

Distribution inference attacks aims to infer statistical properties of data used to train machine learning models. These attacks are sometimes surprisingly potent, as we demonstrated in [previous work](https://uvasrg.github.io/on-the-risks-of-distribution-inference/).

## KL Divergence Attack

Most attacks against distribution inference involve training a meta-classifier, either using model parameters in white-box settings <d-cite key="ganju2018property"></d-cite>, or using model predictions in black-box scenarios <d-cite key="zhang2021leakage"></d-cite>. While other black-box were proposed in our prior work, they are not as accurate as meta-classifier-based methods, and require training shadow models nonetheless <d-cite key="suri2022formalizing"></d-cite>.

We propose a new attack: the KL Divergence Attack. Using some sample of data, the adversary computes predictions on local models from both distributions as well as the victim's model. Then, it uses the prediction probabilities to compute KL divergence between the victim's models and the local models to make its predictions. Our attack outperforms even the current state-of-the-art white-box attacks.

We observe several interesting trends across our experiments. One striking example is that with varying task-property correlation.
While intuition suggests increasing inference leakage with increasing correlation between the classifier's task and the property being inferred, we observe
no such trend:

{% include figure.liquid loading="eager" path="assets/img/ddi/correlation_box.png" class="img-fluid rounded z-depth-1" %}
<div class="caption">
    Distinguishing accuracy for different task-property pairs for Celeb-A dataset for varying correlation. Task-property correlations are: $\approx 0$ (Mouth Slightly Open-Wavy Hair), $\approx 0.14$ (Smiling-Female), $\approx 0.28$ (Female-Young), and $\approx 0.42$ (Mouth Slightly Open-High Cheekbones).
</div>

## Impact of adversary's knowledge

We evaluate inference risk while relaxing a variety of implicit assumptions of the adversary;s knowledge in black-box setups. Concretely, we evaluate label-only API access settings, different victim-adversary feature extractors, and different victim-adversary model architectures.

<table>
<tr>
    <th rowspan="2"> Victim Model </th>
    <th colspan="4"> Adversary Model </th>
</tr>
<tr>
    <th> RF </th>
    <th> LR </th>
    <th> MLP$_2$ </th>
    <th> MLP$_3$ </th>
</tr>
  <tr>
    <td>Random Forest (RF)</td>
    <td> 12.0 </td>
    <td> 1.7 </td>
    <td> 5.4 </td>
    <td> 4.9 </td>
  </tr>
  <tr>
    <td>Linear Regression (LR)</td>
    <td> 13.5 </td>
    <td> 25.9 </td>
    <td> 3.7 </td>
    <td> 5.4 </td>
  </tr>
  <tr>
    <td>Two-layer perceptron (MLP$_2$)</td>
    <td> 0.9 </td>
    <td> 0.3 </td>
    <td> 4.2 </td>
    <td> 4.3 </td>
  </tr>
  <tr>
    <td>Three-layer perceptron (MLP$_3$)</td>
    <td> 0.2 </td>
    <td> 0.3 </td>
    <td> 4.0 </td>
    <td> 3.8 </td>
  </tr>
</table>

Consider inference leakage for the Census19 dataset (table above with mean $n_{leaked}$ values) as an example. Inference risk is significantly higher when the adversary uses models with learning capacity similar to the victim, like both using one of (MLP$_2$, MLP$_3$) or (RF, MLP). Interestingly though, we also observe a sharp increase in inference risk when the victim uses models with low capacity, like LR and RF instead of multi-layer perceptrons.

## Defenses

Finally, we evaluate the effectiveness of some empirical defenses, most of which add noise to the training process.

For instance while inference leakage reduces when the victim utilizes DP, most of the drop in effectiveness comes from a mismatch in the victim's and adversary's training environments:

{% include figure.liquid loading="eager" path="assets/img/ddi/dp_box.png" class="img-fluid rounded z-depth-1" %}
<div class="caption">
    Distinguishing accuracy for different for Census19 (Sex). Attack accuracy drops with stronger DP guarantees i.e. decreasing privacy budget $\epsilon$.
</div>

Compared to an adversary that does not use DP, there is a clear increase in inference risk (mean $n_{leaked}$ increases to 2.9 for $\epsilon=1.0$, and 4.8 for $\epsilon=0.12$ compared to 4.2 without any DP noise).

Our exploration of potential defenses also reveals a strong connection between model generalization and inference risk (as apparent below, for the case of Celeb-A), suggesting that the defenses that do seem to work are attributable to poor model performance, and not something special about the defense itself (like adversarial training or label noise).
</br>


{% include figure.liquid loading="eager" path="assets/img/ddi/generalization_curve.png" class="img-fluid rounded z-depth-1" %}
<div class="caption">
    Mean distinguishing accuracy on Celeb-A (Sex), for varying number of training epochs for victim models. Shaded regions correspond to error bars. Distribution inference risk increases as the model trains, and then starts to decrease as the model starts to overfit.
</div>

## Summary

The general approach to achieve security and privacy for machine-learning models is to add noise, but our evaluations suggest this approach is not a principled or effective defense against distribution inference. The main reductions in inference accuracy that result from these defenses seem to be due to the way they disrupt the model from learning the distribution well.

<b>Paper</b>: [Anshuman Suri](http://anshumansuri.com/), Yifu Lu, Yanjin Chen, [David Evans](http://www.cs.virginia.edu/~evans/). [_Dissecting Distribution Inference_](https://arxiv.org/abs/2212.07591).
In <a href="https://satml.org/"><em>IEEE Conference on Secure and Trustworthy Machine Learning</em></a> (SaTML), 8-10 February 2023.

<b>Code</b>: [https://github.com/iamgroot42/dissecting_distribution_inference](https://github.com/iamgroot42/dissecting_distribution_inference)