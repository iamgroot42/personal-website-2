<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <script>let thunk=()=>{let e=e=>e.trim(),t=e=>e.innerText,n=e=>{let t=e.split(" "),n=t.slice(0,-1).join(" ");return[t.at(-1),n]},a=Array.from(document.getElementsByClassName("author")).map(t).map(e).map(n),r=a[0][0],o=(Array.from(document.getElementsByClassName("affiliation")).filter(e=>"P"===e.nodeName).map(t).map(e),"Advice for working on ML projects"),i="2025",m="October 2, 2025";{let e=a.map(e=>`${e[0]}, ${e[1]}`).join(" and "),t=`\n@article{${(r+i+o.split(" ").slice(0,3).join("")).replace(" ","").replace(/[\p{P}$+<=>^`|~]/gu,"").toLowerCase().trim()},\n  author = {${e}},\n  title = {${o}},\n  year = {${i}},\n  note = {Accessed ${m}},\n  url  = {${window.location.href}}\n}\n  `.trim();document.getElementById("bibtex-box").innerText=t}};document.addEventListener("readystatechange",function(){"complete"===document.readyState&&thunk()});</script> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Advice for working on ML projects | Anshuman Suri </title> <meta name="author" content="Anshuman Suri"> <meta name="description" content="Lessons and recommendations based on my experiences working on ML projects."> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%90%BF%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://anshumansuri.com/blog/2025/ml-considerations/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <script src="/assets/js/distillpub/overrides.js"></script> <d-front-matter> <script async type="text/json">
      {
            "title": "Advice for working on ML projects",
            "description": "Lessons and recommendations based on my experiences working on ML projects.",
            "published": "July 17, 2025",
            "authors": [
              
              {
                "author": "Anshuman Suri",
                "authorURL": "https://anshumansuri.com/",
                "affiliations": [
                  {
                    "name": "Northeastern University",
                    "url": ""
                  }
                ]
              }
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Anshuman</span> Suri </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/news/">news </a> </li> <li class="nav-item "> <a class="nav-link" href="/talks/">talks </a> </li> <li class="nav-item "> <a class="nav-link" href="/service/">service </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>Advice for working on ML projects</h1> <p>Lessons and recommendations based on my experiences working on ML projects.</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div> <a href="#structuring-your-codebase">Structuring your Codebase</a> </div> <div> <a href="#"></a> </div> <ul> <li> <a href="#pip-it">PIP-it!</a> </li> <li> <a href="#dataclasses-are-your-friend">Dataclasses are your friend</a> </li> </ul> <div> <a href="#evaluations">Evaluations</a> </div> <div> <a href="#"></a> </div> <ul> <li> <a href="#how-do-you-like-them-notifications">How do you like them notifications?</a> </li> <li> <a href="#like-a-magic-wand-b">Like a magic WAND(b)</a> </li> </ul> <div> <a href="#i-feel-the-need-the-need-for-speed">I feel the need, the need for speed</a> </div> <div> <a href="#"></a> </div> <ul> <li> <a href="#newbie-gains-with-cupy">Newbie gains with cupy</a> </li> <li> <a href="#compile-can-be-your-friend">Compile can be your friend</a> </li> <li> <a href="#async-transfers">Async transfers</a> </li> <li> <a href="#one-batch-two-batch-penny-and-dime">One batch, two batch, penny and dime</a> </li> </ul> <div> <a href="#slurm-slurm-peralta">SLURM SLURM, Peralta</a> </div> <div> <a href="#takeaways">Takeaways</a> </div> </nav> </d-contents> <p>Working on ML projects in academia (and beyond) often feels like a constant battle between moving fast to test ideas and maintaining enough organization to actually make progress.</p> <p>Based on my experiences, working with collaborators who have diverse coding backgrounds, and—perhaps most importantly—browsing through GitHub repositories of varying quality, I’ve picked up practices and design patterns that have genuinely transformed how I approach ML projects. These aren’t abstract software engineering principles; they’re tested techniques that have saved me from countless headaches and helped me move faster while making fewer mistakes.</p> <p>In this blog, I’ll document the lessons that have made the biggest difference in my day-to-day research workflow. Some might seem obvious in hindsight, others might challenge how you currently organize your work. Either way, I hope they help you spend less time wrestling with logistics and more time focused on the actual science. Note that this is a living document<d-footnote>This means I will update it every now and then based on things I learn</d-footnote>—I’m constantly learning new tricks, and I’ll add them here as I discover what works.</p> <h1 id="structuring-your-codebase">Structuring your Codebase</h1> <p>Some experiments are straightforward and can be self-contained in a file or two. However, most ML projects that span a few weeks or more often end up with growing codebase sizes, with lots of reusable content that can bloat the overall project and lead to subtle inconsistencies when running experiments for different setups.</p> <p>Let’s say you’ve developed a new form of adversarial training and want to run experiments for varying perturbation strengths—including a baseline without any defense. Your project folder might look like this:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>experiments/
├── standard_training.py
├── adversarial_training.py
</code></pre></div></div> <p>Now, during your standard training run, you notice the learning rate is too high (say you started with <code class="language-plaintext highlighter-rouge">1e-3</code>) and reduce it to <code class="language-plaintext highlighter-rouge">1e-4</code>, which fixes the issue. However, since you have separate files for adversarial and standard training, you forget to push the same update to the other file. Your experimental runs now differ not just in the presence/absence of adversarial training, but also in the optimizer hyperparameters—which can have non-trivial impacts on learning dynamics and final results.</p> <p>This example might seem minor, but with growing project sizes and hyperparameters, it’s easy to see how things can go wrong quickly. A straightforward solution would be to have a single <code class="language-plaintext highlighter-rouge">training.py</code> file and support standard training by setting the perturbation budget <code class="language-plaintext highlighter-rouge">epsilon</code> to 0 (or some other sentinel value). It could look something like:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">if</span> <span class="n">args</span><span class="p">.</span><span class="n">epsilon</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
  <span class="c1"># Standard training
</span>  <span class="nf">train</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">optim</span><span class="p">,</span> <span class="n">data_loader</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
  <span class="c1"># Adversarial training
</span>  <span class="nf">adv_train</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">optim</span><span class="p">,</span> <span class="n">data_loader</span><span class="p">,</span> <span class="n">args</span><span class="p">.</span><span class="n">epsilon</span><span class="p">)</span>
</code></pre></div></div> <p>This ensures the <code class="language-plaintext highlighter-rouge">model</code>, <code class="language-plaintext highlighter-rouge">optim</code>, and any other common components are used exactly the same way for both experiments. This approach is intuitive once you think about it, but I’ve seen many researchers<d-footnote>I've been guilty of this at several occasions.</d-footnote> and GitHub projects (especially academic ones) fall into the code duplication trap.</p> <p>Going further with this example, I’ve also seen the equivalent of <code class="language-plaintext highlighter-rouge">adversarial_training_eps4.py</code> in the example above—creating duplicate files with nearly identical code and minor differences (mostly hyperparameters or datasets). This compounds the diverging changes problem and makes it hard to track what’s actually different between experiments.</p> <p>This “single point of failure” approach, in my opinion, is actually useful for research (as long as you catch the bugs, of course). For instance, let’s say all your files use some common evaluation function:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">evaluate</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">loader</span><span class="p">):</span>
  <span class="n">acc</span> <span class="o">=</span> <span class="mi">0</span>
  <span class="k">for</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">loader</span><span class="p">:</span>
    <span class="n">y_pred</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">acc</span> <span class="o">+=</span> <span class="p">(</span><span class="n">y_pred</span> <span class="o">==</span> <span class="n">y</span><span class="p">).</span><span class="nf">sum</span><span class="p">()</span>
  <span class="n">model</span><span class="p">.</span><span class="nf">train</span><span class="p">()</span>
  <span class="k">return</span> <span class="n">acc</span> <span class="o">/</span> <span class="nf">len</span><span class="p">(</span><span class="n">loader</span><span class="p">)</span>
</code></pre></div></div> <p>There are two big issues here:</p> <ul> <li>the accuracy calculation is incorrect: <code class="language-plaintext highlighter-rouge">len(loader)</code> gives the number of batches, not the dataset size, and</li> <li>the model is returned to training mode after evaluation but is never set to eval mode before evaluation begins. This can be especially problematic when the model has data-dependent layers like batch normalization that accumulate statistics.</li> </ul> <p>When the researcher catches this issue, they can at least be assured that whatever mistake they made invalidates all their experiments equally (requiring a complete redo), rather than having the same function in another file, correcting it only there, and making incorrect conclusions about which technique works better.</p> <h2 id="pip-it">PIP-it!</h2> <p>As your codebase grows and you start working on multiple related projects, you’ll inevitably find yourself copy-pasting utility functions, model implementations, and evaluation scripts across different repositories. Let’s say you’ve developed a novel membership inference attack for your latest paper. Six months later, you’re working on a different project and want to use that same attack as a baseline or evaluation metric. What do you do? Copy the files over? Git submodule? Reimplement it from scratch because you can’t find the exact version that worked?</p> <p>This is where creating a proper Python package from your research code can help. Not only does it make your life easier when reusing code across projects, but it also makes it significantly more likely that others will actually use your research. Think about it: would you rather</p> <ul> <li>😵‍💫 download a ZIP file, dig through someone’s experimental scripts, and try to figure out which functions are reusable, or would you prefer to</li> <li>😺 simply <code class="language-plaintext highlighter-rouge">pip install</code> their package and import the functions you need?</li> </ul> <p>The latter is much more appealing, and higher adoption of your methods means more impact. Here’s how this evolution typically looks. You start with a project structure like this:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>membership_inference_project/
├── train_target_model.py
├── run_mia_attack.py
├── utils.py  <span class="c"># Data loading, metrics, plotting</span>
└── models.py <span class="c"># Target models and attack models</span>
</code></pre></div></div> <p>But then you realize that your attack implementation in <code class="language-plaintext highlighter-rouge">models.py</code> is generic enough that others could use it. Instead of letting this code rot in a single project folder, you can structure it as a proper package:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>mia_toolkit/
├── setup.py
├── README.md
├── mia_toolkit/
│   ├── __init__.py
│   ├── attacks/
│   │   ├── __init__.py
│   │   └── membership_inference.py
│   ├── data/
│   │   ├── __init__.py
│   │   └── loaders.py
│   └── utils/
│       ├── __init__.py
│       └── metrics.py
</code></pre></div></div> <p>With a minimal <code class="language-plaintext highlighter-rouge">setup.py</code>, you can now install this directly from GitHub. Note that I used the edit option to install the package with <code class="language-plaintext highlighter-rouge">-e</code> above: this is particularly useful for packages currently under development or when you want to make minimal changes to the code and don’t want to reinstall the package every time you change something!</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># In your new project
</span><span class="n">pip</span> <span class="n">install</span> <span class="o">-</span><span class="n">e</span> <span class="n">git</span><span class="o">+</span><span class="n">https</span><span class="p">:</span><span class="o">//</span><span class="n">github</span><span class="p">.</span><span class="n">com</span><span class="o">/</span><span class="n">yourusername</span><span class="o">/</span><span class="n">mia</span><span class="o">-</span><span class="n">toolkit</span><span class="p">.</span><span class="n">git</span>

<span class="c1"># Clean imports in your code
</span><span class="kn">from</span> <span class="n">mia_toolkit.attacks</span> <span class="kn">import</span> <span class="n">MembershipInferenceAttack</span>
<span class="kn">from</span> <span class="n">mia_toolkit.data</span> <span class="kn">import</span> <span class="n">load_private_dataset</span>
</code></pre></div></div> <p>The benefits extend beyond just your own convenience. When other researchers want to compare against your method, they don’t need to reverse-engineer your experimental scripts—they can simply install your package and focus on the science. This dramatically lowers the barrier to adoption and increases the likelihood that your work will be built upon by others.</p> <div class="alert alert-info" role="alert"> That being said, don't go overboard with this. Not every 50-line script needs to become a package, and there's a delicate balance between making functions generic enough for reuse versus specific enough to actually be useful for your research. I typically package code when I find myself copy-pasting the same utilities across 2-3 projects, or when I think the methods are novel enough that others might want to use them as baselines. </div> <p>A few practical notes: keep your package dependencies minimal and well-documented. I personally try to maintain one conda environment for most of my work, creating new ones only when external baselines require very specific package versions that would otherwise create conflicts.</p> <h2 id="dataclasses-are-your-friend">Dataclasses are your friend</h2> <p>I’ll be honest—this is one of those “do as I say, not as I did” moments. If you look at some of my <a href="https://github.com/suyeecav/model-targeted-poisoning/blob/342f35f7d1204c3a61e84b48c143ec819a55374c/dnn/mtp_dnn.py#L235" rel="external nofollow noopener" target="_blank">older projects</a>, you’ll see argument parsing that looks like this:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">parser</span><span class="p">.</span><span class="nf">add_argument</span><span class="p">(</span><span class="sh">'</span><span class="s">--data_dir</span><span class="sh">'</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="sh">'</span><span class="s">data</span><span class="sh">'</span><span class="p">)</span>
<span class="n">parser</span><span class="p">.</span><span class="nf">add_argument</span><span class="p">(</span><span class="sh">'</span><span class="s">--model_arch</span><span class="sh">'</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="sh">'</span><span class="s">ResNet18</span><span class="sh">'</span><span class="p">)</span>
<span class="n">parser</span><span class="p">.</span><span class="nf">add_argument</span><span class="p">(</span><span class="sh">'</span><span class="s">--lr</span><span class="sh">'</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">float</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
<span class="n">parser</span><span class="p">.</span><span class="nf">add_argument</span><span class="p">(</span><span class="sh">'</span><span class="s">--momentum</span><span class="sh">'</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">float</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>
<span class="n">parser</span><span class="p">.</span><span class="nf">add_argument</span><span class="p">(</span><span class="sh">'</span><span class="s">--weight_decay</span><span class="sh">'</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">float</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mf">5e-4</span><span class="p">)</span>
<span class="n">parser</span><span class="p">.</span><span class="nf">add_argument</span><span class="p">(</span><span class="sh">'</span><span class="s">--batch_size</span><span class="sh">'</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">128</span><span class="p">)</span>
<span class="n">parser</span><span class="p">.</span><span class="nf">add_argument</span><span class="p">(</span><span class="sh">'</span><span class="s">--epochs</span><span class="sh">'</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">200</span><span class="p">)</span>
<span class="n">parser</span><span class="p">.</span><span class="nf">add_argument</span><span class="p">(</span><span class="sh">'</span><span class="s">--poison_lr</span><span class="sh">'</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">float</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
<span class="n">parser</span><span class="p">.</span><span class="nf">add_argument</span><span class="p">(</span><span class="sh">'</span><span class="s">--poison_momentum</span><span class="sh">'</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">float</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>
<span class="n">parser</span><span class="p">.</span><span class="nf">add_argument</span><span class="p">(</span><span class="sh">'</span><span class="s">--poison_epochs</span><span class="sh">'</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>
<span class="n">parser</span><span class="p">.</span><span class="nf">add_argument</span><span class="p">(</span><span class="sh">'</span><span class="s">--target_class</span><span class="sh">'</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">parser</span><span class="p">.</span><span class="nf">add_argument</span><span class="p">(</span><span class="sh">'</span><span class="s">--poison_fraction</span><span class="sh">'</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">float</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
<span class="n">parser</span><span class="p">.</span><span class="nf">add_argument</span><span class="p">(</span><span class="sh">'</span><span class="s">--save_model</span><span class="sh">'</span><span class="p">,</span> <span class="n">action</span><span class="o">=</span><span class="sh">'</span><span class="s">store_true</span><span class="sh">'</span><span class="p">)</span>
<span class="n">parser</span><span class="p">.</span><span class="nf">add_argument</span><span class="p">(</span><span class="sh">'</span><span class="s">--cuda_visible_devices</span><span class="sh">'</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="sh">'</span><span class="s">0</span><span class="sh">'</span><span class="p">)</span>
<span class="n">parser</span><span class="p">.</span><span class="nf">add_argument</span><span class="p">(</span><span class="sh">'</span><span class="s">--random_seed</span><span class="sh">'</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="c1"># ... and about 15 more arguments
</span></code></pre></div></div> <p>This gets unwieldy fast, and worse, it’s error-prone. What if you have both <code class="language-plaintext highlighter-rouge">args.lr</code> and <code class="language-plaintext highlighter-rouge">args.poison_lr</code>? It’s easy to accidentally use the wrong one in your code, especially when you’re debugging at 2 AM<d-footnote>Old habits: Bryan Johnson and Matthew Walker have convinced me to improve my sleeping habits. You should too- it makes a big difference!</d-footnote>.</p> <p>My favorite go-to for these situations is <a href="https://github.com/lebrice/SimpleParsing" rel="external nofollow noopener" target="_blank">SimpleParsing</a>—a wrapper around argparse that leverages Python’s dataclass functionality. Instead of the mess above, you can structure your arguments hierarchically:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">dataclasses</span> <span class="kn">import</span> <span class="n">dataclass</span>
<span class="kn">from</span> <span class="n">simple_parsing</span> <span class="kn">import</span> <span class="n">ArgumentParser</span>

<span class="nd">@dataclass</span>
<span class="k">class</span> <span class="nc">TrainingConfig</span><span class="p">:</span>
    <span class="sh">"""</span><span class="s">Configuration for model training</span><span class="sh">"""</span>
    <span class="n">lr</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span>                    <span class="c1"># Learning rate for optimizer
</span>    <span class="n">momentum</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.9</span>              <span class="c1"># Momentum for SGD optimizer  
</span>    <span class="n">weight_decay</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">5e-4</span>         <span class="c1"># L2 regularization strength
</span>    <span class="n">batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">128</span>              <span class="c1"># Training batch size
</span>    <span class="n">epochs</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">200</span>                  <span class="c1"># Number of training epochs
</span>
<span class="nd">@dataclass</span>
<span class="k">class</span> <span class="nc">PoisonConfig</span><span class="p">:</span>
    <span class="sh">"""</span><span class="s">Configuration for poisoning attack</span><span class="sh">"""</span>
    <span class="n">lr</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span>                    <span class="c1"># Learning rate for poison optimization
</span>    <span class="n">momentum</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.9</span>              <span class="c1"># Momentum for poison optimizer
</span>    <span class="n">epochs</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">50</span>                   <span class="c1"># Poison optimization epochs  
</span>    <span class="n">fraction</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span>              <span class="c1"># Fraction of dataset to poison
</span>    <span class="n">target_class</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span>              <span class="c1"># Target class for attack
</span>
<span class="nd">@dataclass</span>
<span class="k">class</span> <span class="nc">ExperimentConfig</span><span class="p">:</span>
    <span class="sh">"""</span><span class="s">Overall experiment configuration</span><span class="sh">"""</span>
    <span class="n">data_dir</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="sh">"</span><span class="s">data</span><span class="sh">"</span>             <span class="c1"># Path to dataset directory
</span>    <span class="n">model_arch</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="sh">"</span><span class="s">ResNet18</span><span class="sh">"</span>       <span class="c1"># Model architecture to use
</span>    <span class="n">save_model</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="bp">False</span>           <span class="c1"># Whether to save trained model
</span>    <span class="n">random_seed</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span>               <span class="c1"># Random seed for reproducibility
</span>
<span class="n">parser</span> <span class="o">=</span> <span class="nc">ArgumentParser</span><span class="p">()</span>
<span class="n">parser</span><span class="p">.</span><span class="nf">add_arguments</span><span class="p">(</span><span class="n">TrainingConfig</span><span class="p">,</span> <span class="n">dest</span><span class="o">=</span><span class="sh">"</span><span class="s">training</span><span class="sh">"</span><span class="p">)</span>
<span class="n">parser</span><span class="p">.</span><span class="nf">add_arguments</span><span class="p">(</span><span class="n">PoisonConfig</span><span class="p">,</span> <span class="n">dest</span><span class="o">=</span><span class="sh">"</span><span class="s">poison</span><span class="sh">"</span><span class="p">)</span> 
<span class="n">parser</span><span class="p">.</span><span class="nf">add_arguments</span><span class="p">(</span><span class="n">ExperimentConfig</span><span class="p">,</span> <span class="n">dest</span><span class="o">=</span><span class="sh">"</span><span class="s">experiment</span><span class="sh">"</span><span class="p">)</span>

<span class="n">args</span> <span class="o">=</span> <span class="n">parser</span><span class="p">.</span><span class="nf">parse_args</span><span class="p">()</span>
</code></pre></div></div> <p>Now you can run your script with clear, hierarchical arguments:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>python train.py <span class="nt">--training</span>.lr 0.01 <span class="nt">--poison</span>.lr 0.1 <span class="nt">--experiment</span>.data_dir /path/to/data
</code></pre></div></div> <p>The benefits are immediately obvious. No more confusion between <code class="language-plaintext highlighter-rouge">args.lr</code> and <code class="language-plaintext highlighter-rouge">args.poison_lr</code>—it’s now <code class="language-plaintext highlighter-rouge">args.training.lr</code> versus <code class="language-plaintext highlighter-rouge">args.poison.lr</code>. The hierarchy makes it crystal clear which learning rate you’re referring to, and the docstrings serve double duty as both code documentation and command-line help text.</p> <p>But the real magic happens when you start reusing these configurations across files. Instead of copy-pasting argument definitions (and inevitably introducing inconsistencies), you can simply import your dataclasses:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Your project structure</span>
...
├── configs.py       <span class="c"># All dataclass definitions</span>
├── train_model.py
├── evaluate_model.py
└── run_attack.py
</code></pre></div></div> <p>Each script can import exactly the configurations it needs:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># In train_model.py</span>
from configs import TrainingConfig, ExperimentConfig

<span class="c"># In run_attack.py  </span>
from configs import PoisonConfig, ExperimentConfig

<span class="c"># In evaluate_model.py</span>
from configs import ExperimentConfig
</code></pre></div></div> <p>This ensures that when you update the default learning rate in <code class="language-plaintext highlighter-rouge">TrainingConfig</code>, it’s automatically reflected across all scripts that use it. No more hunting through multiple files to make sure you’ve updated the same hyperparameter everywhere.</p> <p>SimpleParsing also handles saving and loading configurations to/from YAML or JSON files, which makes experiment reproduction trivial. Instead of trying to remember the exact command-line arguments you used three weeks ago, you can simply:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Save your current config to configs/experiment_1.yaml</span>
<span class="c"># Reproduce the exact same experiment later</span>
python train.py <span class="nt">--config_path</span> configs/experiment_1.yaml
</code></pre></div></div> <h1 id="evaluations">Evaluations</h1> <p>Efficient experiment management can mean the difference between spending hours (or days) babysitting jobs and actually having time to think and do research—using the right tools lets you offload the busywork and focus on what matters.</p> <h2 id="how-do-you-like-them-notifications">How do you like them notifications?</h2> <p>Picture this: you start a training run that’s supposed to take 6 hours, close your laptop, and go about your day. Six hours later, you eagerly check back expecting to see beautiful loss curves, only to discover your script crashed 20 minutes in due to a CUDA out-of-memory error 😱. Sound familiar?</p> <p>Most ML experiments take hours or even days to complete, and the traditional approach of estimating runtime with <code class="language-plaintext highlighter-rouge">tqdm</code> and checking the ETA only gets you so far. What you really need is to know the moment your experiment finishes—or more importantly, when it crashes.</p> <p><a href="https://github.com/huggingface/knockknock" rel="external nofollow noopener" target="_blank">knockknock</a> from HuggingFace has been an absolute lifesaver for this! It’s a simple Python package that sends you notifications when your experiments complete or fail. The setup is straightforward:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pip <span class="nb">install </span>knockknock
</code></pre></div></div> <p>You can use it as a decorator directly in your code but honestly, I prefer the command-line approach since it doesn’t require modifying your existing code. You can set up a simple wrapper script in your <code class="language-plaintext highlighter-rouge">~/bin</code> directory:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">#!/bin/bash</span>
<span class="c"># Save this as ~/bin/knocky and make it executable with chmod +x</span>
<span class="c"># Example below is for Telegram</span>
knockknock telegram <span class="se">\</span>
    <span class="nt">--token</span> YOUR_TELEGRAM_TOKEN <span class="se">\</span>
    <span class="nt">--chat-id</span> YOUR_CHAT_ID <span class="se">\</span>
    <span class="s2">"</span><span class="nv">$@</span><span class="s2">"</span>
</code></pre></div></div> <p>Now you can run any experiment with notifications by simply prefixing your command:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Instead of: python train_resnet.py --epochs 200</span>
knocky python train_resnet.py <span class="nt">--epochs</span> 200
</code></pre></div></div> <p>The beauty is that you get notifications both when your script completes successfully and when it crashes with an error. No more checking in every few hours or trying to estimate completion times. I personally use Telegram<d-footnote>Setup details for tokens and bot ID here: https://github.com/huggingface/knockknock?tab=readme-ov-file#telegram</d-footnote> since it’s reliable and I always have it on my phone, but knockknock supports Slack, Discord, email, and several other platforms.</p> <p>This simple change has saved me countless hours of babysitting experiments (or logging in anxiously every 1-2 hours). Plus, there’s something deeply satisfying about getting a notification that your model finished training while you’re grabbing coffee or on your way to work.</p> <h2 id="like-a-magic-wandb">Like a magic WAND(b)</h2> <p>Remember when comparing different experimental runs meant opening multiple terminal windows, squinting at loss values printed to stdout, and trying to remember which combination of hyperparameters gave you that promising result from last Tuesday? Or frantically searching through your bash history because you forgot the exact arguments you used for your best-performing model?</p> <p>I used to have training scripts that would dump metrics to text files, create matplotlib plots locally, and leave me manually tracking which experiment was which:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">train_epoch</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">loader</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">epoch</span><span class="p">,</span> <span class="n">exp_name</span><span class="p">):</span>
    <span class="n">total_loss</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">loader</span><span class="p">):</span>
        <span class="c1"># ... training code ...
</span>        
        <span class="c1"># Manual logging (the old way)
</span>        <span class="k">if</span> <span class="n">batch_idx</span> <span class="o">%</span> <span class="mi">100</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">Epoch </span><span class="si">{</span><span class="n">epoch</span><span class="si">}</span><span class="s">, Batch </span><span class="si">{</span><span class="n">batch_idx</span><span class="si">}</span><span class="s">, Loss: </span><span class="si">{</span><span class="n">loss</span><span class="p">.</span><span class="nf">item</span><span class="p">()</span><span class="si">:</span><span class="p">.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="sh">'</span><span class="p">)</span>
            
            <span class="c1"># Dump to files for later analysis
</span>            <span class="k">with</span> <span class="nf">open</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">logs/</span><span class="si">{</span><span class="n">exp_name</span><span class="si">}</span><span class="s">_loss.txt</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">a</span><span class="sh">'</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
                <span class="n">f</span><span class="p">.</span><span class="nf">write</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="si">{</span><span class="n">epoch</span><span class="si">}</span><span class="s">,</span><span class="si">{</span><span class="n">batch_idx</span><span class="si">}</span><span class="s">,</span><span class="si">{</span><span class="n">loss</span><span class="p">.</span><span class="nf">item</span><span class="p">()</span><span class="si">}</span><span class="se">\n</span><span class="sh">'</span><span class="p">)</span>
            
            <span class="c1"># Save plots occasionally
</span>            <span class="k">if</span> <span class="n">batch_idx</span> <span class="o">%</span> <span class="mi">1000</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">()</span>
                <span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">losses</span><span class="p">)</span>
                <span class="n">plt</span><span class="p">.</span><span class="nf">savefig</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">plots/</span><span class="si">{</span><span class="n">exp_name</span><span class="si">}</span><span class="s">_loss_epoch_</span><span class="si">{</span><span class="n">epoch</span><span class="si">}</span><span class="s">.png</span><span class="sh">'</span><span class="p">)</span>
                <span class="n">plt</span><span class="p">.</span><span class="nf">close</span><span class="p">()</span>
</code></pre></div></div> <p>Then you end up with a mess of files like <code class="language-plaintext highlighter-rouge">resnet_lr001_wd0001_loss.txt</code> and <code class="language-plaintext highlighter-rouge">resnet_lr01_wd0005_loss.txt</code>, and good luck remembering which file corresponds to which exact experimental setup three weeks later.</p> <p>Enter <a href="https://wandb.ai/" rel="external nofollow noopener" target="_blank">Weights &amp; Biases (wandb)</a>—hands down the biggest<d-footnote>TensorBoard and MLflow are good alternatives too.</d-footnote> game-changer for my research workflow:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">wandb</span>

<span class="c1"># Initialize once at the start of your script
</span><span class="n">wandb</span><span class="p">.</span><span class="nf">init</span><span class="p">(</span>
    <span class="n">project</span><span class="o">=</span><span class="sh">"</span><span class="s">my-awesome-research</span><span class="sh">"</span><span class="p">,</span>
    <span class="n">config</span><span class="o">=</span><span class="p">{</span>
        <span class="sh">"</span><span class="s">learning_rate</span><span class="sh">"</span><span class="p">:</span> <span class="n">args</span><span class="p">.</span><span class="n">lr</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">weight_decay</span><span class="sh">"</span><span class="p">:</span> <span class="n">args</span><span class="p">.</span><span class="n">weight_decay</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">architecture</span><span class="sh">"</span><span class="p">:</span> <span class="n">args</span><span class="p">.</span><span class="n">model</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">dataset</span><span class="sh">"</span><span class="p">:</span> <span class="n">args</span><span class="p">.</span><span class="n">dataset</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">batch_size</span><span class="sh">"</span><span class="p">:</span> <span class="n">args</span><span class="p">.</span><span class="n">batch_size</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">epochs</span><span class="sh">"</span><span class="p">:</span> <span class="n">args</span><span class="p">.</span><span class="n">epochs</span><span class="p">,</span>
    <span class="p">}</span>
<span class="p">)</span>

<span class="k">def</span> <span class="nf">train_epoch</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">loader</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">epoch</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">loader</span><span class="p">):</span>
        <span class="c1"># ... training code ...
</span>        
        <span class="c1"># That's it! One line of logging
</span>        <span class="n">wandb</span><span class="p">.</span><span class="nf">log</span><span class="p">({</span>
            <span class="sh">"</span><span class="s">train/loss</span><span class="sh">"</span><span class="p">:</span> <span class="n">loss</span><span class="p">.</span><span class="nf">item</span><span class="p">(),</span>
            <span class="sh">"</span><span class="s">train/accuracy</span><span class="sh">"</span><span class="p">:</span> <span class="n">accuracy</span><span class="p">,</span>
            <span class="sh">"</span><span class="s">epoch</span><span class="sh">"</span><span class="p">:</span> <span class="n">epoch</span><span class="p">,</span>
            <span class="sh">"</span><span class="s">learning_rate</span><span class="sh">"</span><span class="p">:</span> <span class="n">optimizer</span><span class="p">.</span><span class="n">param_groups</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="sh">'</span><span class="s">lr</span><span class="sh">'</span><span class="p">]</span>
        <span class="p">})</span>

<span class="c1"># Automatically track your model's gradients and parameters
</span><span class="n">wandb</span><span class="p">.</span><span class="nf">watch</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">log_freq</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
</code></pre></div></div> <div class="row mt-1"> <div class="col-sm mt-2 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/considerations/console-480.webp 480w,/assets/img/considerations/console-800.webp 800w,/assets/img/considerations/console-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/considerations/console.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>The magic isn’t just in the simplicity of logging—it’s in what wandb does with that information. Every single run gets tracked with:</p> <ul> <li> <strong>All your hyperparameters</strong>: No more “what learning rate did I use again?”</li> <li> <strong>Real-time metrics</strong>: Plots update live as your model trains</li> <li> <strong>System monitoring</strong>: GPU utilization, memory usage, CPU stats</li> <li> <strong>Code tracking</strong>: Git commit hash, diff, and even the exact command you ran</li> <li> <strong>Reproducibility</strong>: One-click to see the exact environment and arguments</li> </ul> <p>Instead of manually plotting loss curves from different text files, you can select multiple runs in the wandb interface and overlay their metrics instantly. Need to see how learning rate affects convergence? Select all runs with different LRs and compare their loss curves side-by-side. Want to find your best-performing model from the last month? Sort by validation accuracy and boom—there it is, with all the hyperparameters clearly listed.</p> <p>You can even log media directly:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Log images, plots, and even 3D visualizations
</span><span class="n">wandb</span><span class="p">.</span><span class="nf">log</span><span class="p">({</span>
    <span class="sh">"</span><span class="s">predictions</span><span class="sh">"</span><span class="p">:</span> <span class="n">wandb</span><span class="p">.</span><span class="nc">Image</span><span class="p">(</span><span class="n">prediction_plot</span><span class="p">),</span>
    <span class="sh">"</span><span class="s">confusion_matrix</span><span class="sh">"</span><span class="p">:</span> <span class="n">wandb</span><span class="p">.</span><span class="n">plot</span><span class="p">.</span><span class="nf">confusion_matrix</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">labels</span><span class="p">),</span>
    <span class="sh">"</span><span class="s">sample_predictions</span><span class="sh">"</span><span class="p">:</span> <span class="p">[</span><span class="n">wandb</span><span class="p">.</span><span class="nc">Image</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">caption</span><span class="o">=</span><span class="sa">f</span><span class="sh">"</span><span class="s">Pred: </span><span class="si">{</span><span class="n">pred</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span> 
                          <span class="k">for</span> <span class="n">img</span><span class="p">,</span> <span class="n">pred</span> <span class="ow">in</span> <span class="nf">zip</span><span class="p">(</span><span class="n">sample_images</span><span class="p">,</span> <span class="n">predictions</span><span class="p">)]</span>
<span class="p">})</span>
</code></pre></div></div> <p>The filtering and search capabilities are phenomenal too. You can filter runs by any combination of hyperparameters, metric ranges, or tags. Looking for all ResNet experiments with learning rate &gt; 0.01 that achieved &gt;90% accuracy? Just use the built-in filters. This has saved me countless hours of digging through experimental logs<d-footnote>The free tier gives you unlimited personal projects and up to 100GB of storage, which is more than enough for most academic work</d-footnote>.</p> <div class="row mt-1"> <div class="col-sm mt-2 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/considerations/filter-480.webp 480w,/assets/img/considerations/filter-800.webp 800w,/assets/img/considerations/filter-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/considerations/filter.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>Since adopting wandb, I’ve never lost track of an experimental run, never forgotten which hyperparameters produced good results, and never had to manually create comparison plots again. It’s one of those tools that immediately makes you wonder how you ever lived without it.</p> <h1 id="i-feel-the-need-the-need-for-speed">I feel the need, the need for speed</h1> <p>So far, I’ve focused on ways to keep your codebase and experiments organized—making it easier to run, track, and reproduce results. But once that’s in place, it’s worth considering tweaks that can actually speed up each individual experiment, especially when training times start to add up.</p> <h2 id="newbie-gains-with-cupy">Newbie gains with cupy</h2> <p>If you find yourself writing a lot of numpy code for data wrangling or preprocessing, it’s worth knowing that numpy itself is strictly CPU-bound. For larger arrays or more intensive computations, this can become a bottleneck. CuPy is a drop-in replacement for numpy that runs operations on NVIDIA GPUs, often requiring only a change from <code class="language-plaintext highlighter-rouge">import numpy as np</code> to <code class="language-plaintext highlighter-rouge">import cupy as cp</code>.</p> <p>For example:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">cupy</span> <span class="k">as</span> <span class="n">cp</span>

<span class="c1"># Allocate arrays on the GPU
</span><span class="n">x</span> <span class="o">=</span> <span class="n">cp</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mi">1000000</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">cp</span><span class="p">.</span><span class="nf">sin</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="c1"># Operations are performed on the GPU
</span><span class="n">result</span> <span class="o">=</span> <span class="n">cp</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
</code></pre></div></div> <p>Most common numpy functions are supported, and the syntax is nearly identical. The main caveat is that you’ll need to move data between CPU and GPU explicitly, and not every numpy feature is available. But for heavy array computations, switching to CuPy can save a surprising amount of time compared to pure numpy.</p> <div class="row mt-1"> <div class="col-sm mt-2 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/considerations/cupy_speedups.webp" sizes="95vw"></source> <img src="/assets/img/considerations/cupy_speedups.webp" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> Source: https://medium.com/rapids-ai/single-gpu-cupy-speedups-ea99cbbb0cbb </div> </div> </div> <h2 id="compile-can-be-your-friend">Compile can be your friend</h2> <p><code class="language-plaintext highlighter-rouge">torch.compile()</code> is one of those features that’s worth trying out. The idea is simple: you wrap your model (or even just a function) with <code class="language-plaintext highlighter-rouge">torch.compile()</code>, and PyTorch will try to optimize it under the hood—things like kernel fusion, better graph execution, and other tricks that can speed up training and inference. You don’t need to change your code structure or rewrite your model; it’s meant to be a drop-in improvement.</p> <p>Here’s a minimal example:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">torch</span>
<span class="kn">import</span> <span class="n">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">(</span>
    <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">256</span><span class="p">),</span>
    <span class="n">nn</span><span class="p">.</span><span class="nc">ReLU</span><span class="p">(),</span>
    <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="p">)</span>

<span class="c1"># Just wrap your model
</span><span class="n">compiled_model</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">compile</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>

<span class="c1"># Use as usual
</span><span class="k">for</span> <span class="n">data</span><span class="p">,</span> <span class="n">target</span> <span class="ow">in</span> <span class="n">loader</span><span class="p">:</span>
    <span class="n">output</span> <span class="o">=</span> <span class="nf">compiled_model</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="nf">criterion</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
    <span class="c1"># ...existing training code...
</span></code></pre></div></div> <p>You can also compile arbitrary functions, not just models:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">custom_forward</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="c1"># ...some tensor ops...
</span>    <span class="k">return</span> <span class="n">x</span> <span class="o">*</span> <span class="mi">2</span> <span class="o">+</span> <span class="n">torch</span><span class="p">.</span><span class="nf">sin</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="n">compiled_fn</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">compile</span><span class="p">(</span><span class="n">custom_forward</span><span class="p">)</span>
<span class="n">result</span> <span class="o">=</span> <span class="nf">compiled_fn</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">128</span><span class="p">))</span>
</code></pre></div></div> <p>The main tradeoff is that the first time you run a compiled model or function, it will be noticeably slower—PyTorch is tracing and optimizing the computation graph. For workloads where you only run a few batches, this overhead isn’t worth it. But if you’re training/evaluating for multiple iterations/batches, the initial cost gets amortized, and you can see real speedups (sometimes 20-30% or more, depending on the model and hardware).</p> <h2 id="async-transfers">Async transfers</h2> <p>You’ve probably noticed that your GPU utilization sometimes hovers around 70-80% instead of the near-100% you’d expect, even when your batch size and model complexity seem reasonable. The hidden culprit is often data transfer time between CPU and GPU—every <code class="language-plaintext highlighter-rouge">.to(device)</code> call is a synchronization point by default, meaning your expensive GPU sits idle waiting for data to crawl over the PCIe bus.</p> <p>The easiest win is enabling pinned memory in your DataLoader, which uses page-locked host memory for much faster transfers:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Simple change with immediate benefits
</span><span class="n">train_loader</span> <span class="o">=</span> <span class="nc">DataLoader</span><span class="p">(</span>
    <span class="n">dataset</span><span class="p">,</span> 
    <span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> 
    <span class="n">pin_memory</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>  <span class="c1"># This alone can give 20-30% speedup
</span>    <span class="n">num_workers</span><span class="o">=</span><span class="mi">4</span>
<span class="p">)</span>

<span class="c1"># Now use non-blocking transfers
</span><span class="k">for</span> <span class="n">data</span><span class="p">,</span> <span class="n">target</span> <span class="ow">in</span> <span class="n">train_loader</span><span class="p">:</span>
    <span class="n">data</span> <span class="o">=</span> <span class="n">data</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">,</span> <span class="n">non_blocking</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="n">target</span> <span class="o">=</span> <span class="n">target</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">,</span> <span class="n">non_blocking</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    
    <span class="n">output</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="nf">criterion</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
</code></pre></div></div> <p>The real benefit comes when you can overlap transfers with computation:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># X is large, Y is small
</span><span class="n">x</span> <span class="o">=</span> <span class="n">large_tensor</span><span class="p">.</span><span class="nf">pin_memory</span><span class="p">()</span>  <span class="c1"># e.g., batch of images
</span><span class="n">y</span> <span class="o">=</span> <span class="n">small_tensor</span><span class="p">.</span><span class="nf">pin_memory</span><span class="p">()</span>  <span class="c1"># e.g., single image or metadata
</span>
<span class="c1"># Start transferring the large tensor asynchronously
</span><span class="n">x_gpu</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="nf">cuda</span><span class="p">(</span><span class="n">non_blocking</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="c1"># While X is transferring, process Y
</span><span class="n">y_gpu</span> <span class="o">=</span> <span class="n">y</span><span class="p">.</span><span class="nf">cuda</span><span class="p">()</span>  <span class="c1"># Small, transfers quickly
</span><span class="n">output_y</span> <span class="o">=</span> <span class="nf">model2</span><span class="p">(</span><span class="n">y_gpu</span><span class="p">)</span>

<span class="c1"># By now X should be ready on GPU
</span><span class="n">output_x</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="n">x_gpu</span><span class="p">)</span>
</code></pre></div></div> <p>The key insight is using the time it takes to transfer large data to do other useful work—processing smaller tensors, running computations, or preparing the next batch<d-footnote>The PyTorch tutorial on pinned memory has more details on the underlying mechanics: https://docs.pytorch.org/tutorials/intermediate/pinmem_nonblock.html</d-footnote>.</p> <p>Async transfers only help when the next operation doesn’t immediately depend on the transferred data. If you call <code class="language-plaintext highlighter-rouge">model(data)</code> right after <code class="language-plaintext highlighter-rouge">.to(device, non_blocking=True)</code>, PyTorch will still wait for the transfer to complete before starting the forward pass.</p> <p>The real gotcha comes when transferring data back to CPU, especially with explicit async calls:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">save_predictions</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">dataloader</span><span class="p">):</span>
    <span class="n">predictions</span> <span class="o">=</span> <span class="p">[]</span>
    
    <span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="nf">no_grad</span><span class="p">():</span>
        <span class="k">for</span> <span class="n">data</span><span class="p">,</span> <span class="n">target</span> <span class="ow">in</span> <span class="n">dataloader</span><span class="p">:</span>
            <span class="n">output</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="n">data</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">))</span>
            <span class="n">pred</span> <span class="o">=</span> <span class="n">output</span><span class="p">.</span><span class="nf">argmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
            
            <span class="c1"># If you use non_blocking=True here, this becomes dangerous:
</span>            <span class="n">pred_cpu</span> <span class="o">=</span> <span class="n">pred</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="sh">'</span><span class="s">cpu</span><span class="sh">'</span><span class="p">,</span> <span class="n">non_blocking</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
            
            <span class="c1"># BUG: numpy() might execute before transfer completes!
</span>            <span class="n">predictions</span><span class="p">.</span><span class="nf">extend</span><span class="p">(</span><span class="n">pred_cpu</span><span class="p">.</span><span class="nf">numpy</span><span class="p">())</span>  <span class="c1"># Potential garbage data
</span></code></pre></div></div> <div class="alert alert-danger" role="alert"> The issue arises because when you explicitly use `non_blocking=True` for GPU→CPU transfers, the CPU doesn't wait for the transfer to complete. Accessing the data (like with `.numpy()`) before the transfer finishes gives you garbage. The fixes are straightforward: </div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Option 1: Don't use non_blocking for GPU→CPU (default behavior)
</span><span class="n">pred_cpu</span> <span class="o">=</span> <span class="n">pred</span><span class="p">.</span><span class="nf">cpu</span><span class="p">()</span>  <span class="c1"># Synchronous by default
</span><span class="n">predictions</span><span class="p">.</span><span class="nf">extend</span><span class="p">(</span><span class="n">pred_cpu</span><span class="p">.</span><span class="nf">numpy</span><span class="p">())</span>

<span class="c1"># Option 2: If you do use non_blocking, explicitly synchronize
</span><span class="n">pred_cpu</span> <span class="o">=</span> <span class="n">pred</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="sh">'</span><span class="s">cpu</span><span class="sh">'</span><span class="p">,</span> <span class="n">non_blocking</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">synchronize</span><span class="p">()</span>  <span class="c1"># Wait for all GPU operations to complete
</span><span class="n">predictions</span><span class="p">.</span><span class="nf">extend</span><span class="p">(</span><span class="n">pred_cpu</span><span class="p">.</span><span class="nf">numpy</span><span class="p">())</span>

<span class="c1"># Option 3: Accumulate on GPU, transfer once at the end
</span><span class="n">all_preds</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">cat</span><span class="p">(</span><span class="n">gpu_predictions</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">).</span><span class="nf">cpu</span><span class="p">().</span><span class="nf">numpy</span><span class="p">()</span>
</code></pre></div></div> <p>The key insight is that async transfers shine when you can overlap them with computation that doesn’t depend on the transferred data. Combined with pinned memory, this can substantially improve throughput for data-heavy workloads.</p> <h2 id="one-batch-two-batch-penny-and-dime">One batch, two batch, penny and dime</h2> <p>When it comes to inference, there’s rarely a good reason not to push your GPU memory usage as much as possible. The ideal, principled approach is to calculate the maximum batch size your model and script can support, given the memory constraints. In practice, though, many tend to be a bit lazy here—usually starting with a conservative batch size and gradually increasing it until I hit an OOM error.</p> <p>A good sweet spot is to try and empirically infer the relationship between batch size and GPU memory consumption for your specific setup. This helps avoid surprises, especially when switching models or datasets. If you want to get a sense of your memory usage patterns, I’ve found it useful to track GPU memory throughout the experiment. I wrote a <a href="https://gist.github.com/iamgroot42/5f1f33e39621e545c621e90472b649d3" rel="external nofollow noopener" target="_blank">barebones utility script</a>) that monitors <code class="language-plaintext highlighter-rouge">nvidia-smi</code> during your run and summarizes memory usage at the end. This makes it easy to spot the peak usage, debug unexpected spikes, or decide if you need to adjust batch sizes for certain inputs (e.g., truncate long sequences, partition batches for variable-length data).</p> <h1 id="slurm-slurm-peralta">SLURM SLURM, Peralta</h1> <p>If you have access to a SLURM cluster, you’re sitting on a goldmine for running ML experiments—but most people use it like an overpowered SSH session. Instead of thinking “how do I run this one experiment on SLURM?”, start thinking “how do I run all my experiments efficiently?”</p> <p>Here’s what the inefficient approach looks like. You want to test your new membership inference attack:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>sbatch experiment_1.slurm
<span class="c"># Wait... check results... then:</span>
sbatch experiment_1.slurm
<span class="c"># Wait... check results... then:</span>
sbatch experiment_1.slurm
<span class="c"># And so on...</span>
</code></pre></div></div> <p>There is no reason to submit jobs only when previous ones finish- in the absolute worst case (SLURM is extra busy, your jobs have very low priority in the queue), your jobs may actually end up running one after the other but in the average/best case, they will all run in parallel. tl;dr let the SLURM scheduler worry about scheduling the jobs- just submit them all at once!</p> <p>One thing that is especially helpful here is job arrays—the feature that transforms SLURM from a glorified remote desktop into a proper experiment manager:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># One command to rule them all</span>
sbatch <span class="nt">--array</span><span class="o">=</span>0-5 run_experiment.slurm
</code></pre></div></div> <p>This single command launches 6 jobs simultaneously (indices 0 through 5), each with a unique <code class="language-plaintext highlighter-rouge">SLURM_ARRAY_TASK_ID</code> that your script can use to determine which specific experiment to run. Inside your <code class="language-plaintext highlighter-rouge">run_experiment.slurm</code>, you map the task ID to experimental parameters:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">#!/bin/bash</span>
<span class="c">#SBATCH --ntasks=1</span>
<span class="c">#SBATCH --mem=32G</span>
<span class="c">#SBATCH --gres=gpu:1</span>
<span class="c">#SBATCH --time=2:00:00</span>
<span class="c">#SBATCH --output=logs/exp_%A_%a.out</span>
<span class="c">#SBATCH --error=logs/exp_%A_%a.err</span>

<span class="c"># Define your experimental grid</span>
<span class="nv">MODELS</span><span class="o">=(</span>resnet18 resnet50 vgg16<span class="o">)</span>
<span class="nv">DATASETS</span><span class="o">=(</span>cifar10 imagenet<span class="o">)</span>

<span class="c"># Calculate which model and dataset to use</span>
<span class="nv">MODEL_IDX</span><span class="o">=</span><span class="k">$((</span>SLURM_ARRAY_TASK_ID <span class="o">/</span> <span class="m">2</span><span class="k">))</span>
<span class="nv">DATASET_IDX</span><span class="o">=</span><span class="k">$((</span>SLURM_ARRAY_TASK_ID <span class="o">%</span> <span class="m">2</span><span class="k">))</span>

<span class="nv">MODEL</span><span class="o">=</span><span class="k">${</span><span class="nv">MODELS</span><span class="p">[</span><span class="nv">$MODEL_IDX</span><span class="p">]</span><span class="k">}</span>
<span class="nv">DATASET</span><span class="o">=</span><span class="k">${</span><span class="nv">DATASETS</span><span class="p">[</span><span class="nv">$DATASET_IDX</span><span class="p">]</span><span class="k">}</span>

<span class="nb">echo</span> <span class="s2">"Running experiment: </span><span class="nv">$MODEL</span><span class="s2"> on </span><span class="nv">$DATASET</span><span class="s2">"</span>
python run_mia_attack.py <span class="nt">--model</span> <span class="nv">$MODEL</span> <span class="nt">--dataset</span> <span class="nv">$DATASET</span>
</code></pre></div></div> <p>The <code class="language-plaintext highlighter-rouge">%A_%a</code> in the output files gives you the job array ID and task ID, so you get separate log files like <code class="language-plaintext highlighter-rouge">exp_12345_0.out</code>, <code class="language-plaintext highlighter-rouge">exp_12345_1.out</code>, etc. This makes debugging individual runs much easier than having everything mixed together.</p> <p>But job arrays aren’t just for hyperparameter sweeps. I use them for:</p> <ul> <li> <strong>Testing different baselines</strong>: Run your method against 10 different attack baselines simultaneously</li> <li> <strong>Cross-dataset evaluation</strong>: Evaluate the same model on multiple datasets</li> <li> <strong>Ablation studies</strong>: Test different components of your method (with/without data augmentation, different loss functions, etc.)</li> <li> <strong>Robustness testing</strong>: Same experiment with different random seeds to check variance</li> </ul> <p>The real power comes when you need to run many variations. Want to test 5 models × 3 datasets × 4 random seeds = 60 experiments? Instead of submitting jobs one by one over several days, you submit one array job and walk away:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>sbatch <span class="nt">--array</span><span class="o">=</span>0-59 comprehensive_eval.slurm
</code></pre></div></div> <p>Your script maps the 60 task IDs to the appropriate combinations:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">MODELS</span><span class="o">=(</span>resnet18 resnet50 vgg16 densenet alexnet<span class="o">)</span>
<span class="nv">DATASETS</span><span class="o">=(</span>cifar10 cifar100 imagenet<span class="o">)</span>
<span class="nv">SEEDS</span><span class="o">=(</span>42 123 456 789<span class="o">)</span>

<span class="c"># Extract indices from SLURM_ARRAY_TASK_ID</span>
<span class="nv">SEED_IDX</span><span class="o">=</span><span class="k">$((</span>SLURM_ARRAY_TASK_ID <span class="o">%</span> <span class="m">4</span><span class="k">))</span>
<span class="nv">DATASET_IDX</span><span class="o">=</span><span class="k">$((</span><span class="o">(</span>SLURM_ARRAY_TASK_ID <span class="o">/</span> <span class="m">4</span><span class="o">)</span> <span class="o">%</span> <span class="m">3</span><span class="k">))</span>
<span class="nv">MODEL_IDX</span><span class="o">=</span><span class="k">$((</span>SLURM_ARRAY_TASK_ID <span class="o">/</span> <span class="m">12</span><span class="k">))</span>

<span class="nv">MODEL</span><span class="o">=</span><span class="k">${</span><span class="nv">MODELS</span><span class="p">[</span><span class="nv">$MODEL_IDX</span><span class="p">]</span><span class="k">}</span>
<span class="nv">DATASET</span><span class="o">=</span><span class="k">${</span><span class="nv">DATASETS</span><span class="p">[</span><span class="nv">$DATASET_IDX</span><span class="p">]</span><span class="k">}</span>
<span class="nv">SEED</span><span class="o">=</span><span class="k">${</span><span class="nv">SEEDS</span><span class="p">[</span><span class="nv">$SEED_IDX</span><span class="p">]</span><span class="k">}</span>
</code></pre></div></div> <p>A few practical tips that have saved me headaches:</p> <p><strong>Resource sizing</strong>: Don’t request more resources than you need. If your job only uses 8GB of memory, don’t request 64GB—you’ll wait longer in the queue and waste allocation budget. I usually run a few experiments locally first to get a rough estimate of memory and runtime requirements.</p> <p><strong>Smart array sizing</strong>: Instead of submitting massive arrays (like <code class="language-plaintext highlighter-rouge">--array=0-999</code>), consider breaking them into smaller chunks (<code class="language-plaintext highlighter-rouge">--array=0-99</code>, then <code class="language-plaintext highlighter-rouge">--array=100-199</code>, etc.). This gives you more flexibility if you need to cancel some jobs or if you discover an issue with your setup early on.</p> <p><strong>Checkpoint your work</strong>: For longer experiments, save intermediate results. SLURM has time limits, and there’s nothing worse than losing 8 hours of training because your job hit the wall time. A simple checkpoint every epoch can save you from starting over.</p> <p>As I mentioned in my <a href="https://www.anshumansuri.com/blog/2022/uva-rivanna/" rel="external nofollow noopener" target="_blank">earlier post</a> about SLURM, there are plenty of other useful features and cluster-specific quirks to learn. But mastering job arrays alone will transform how you approach large-scale experimentation.</p> <h1 id="takeaways">Takeaways</h1> <p>Most of this post is just a collection of practical habits and tools that have made my ML workflow less painful and more reproducible. If you have other tricks or approaches that work well for you, I’d be interested to hear about them—feel free to reach out or contribute to the post <a href="https://github.com/iamgroot42/personal-website-2" rel="external nofollow noopener" target="_blank">directly with a PR</a>!</p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/"></d-bibliography> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Anshuman Suri. Last updated: October 02, 2025. </div> </footer> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"about",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-publications",title:"publications",description:"",section:"Navigation",handler:()=>{window.location.href="/publications/"}},{id:"nav-blog",title:"blog",description:"",section:"Navigation",handler:()=>{window.location.href="/blog/"}},{id:"nav-news",title:"news",description:"",section:"Navigation",handler:()=>{window.location.href="/news/"}},{id:"nav-talks",title:"talks",description:"",section:"Navigation",handler:()=>{window.location.href="/talks/"}},{id:"nav-service",title:"service",description:"",section:"Navigation",handler:()=>{window.location.href="/service/"}},{id:"post-advice-for-working-on-ml-projects",title:"Advice for working on ML projects",description:"Lessons and recommendations based on my experiences working on ML projects.",section:"Posts",handler:()=>{window.location.href="/blog/2025/ml-considerations/"}},{id:"post-reassessing-emnlp-2024-s-best-paper-does-divergence-based-calibration-for-membership-inference-attacks-hold-up",title:"Reassessing EMNLP 2024\u2019s Best Paper: Does Divergence-Based Calibration for Membership Inference Attacks Hold...",description:"TL;DR: No. A critical analysis of the EMNLP Best Paper proposing a divergence-based calibration for Membership Inference Attacks (MIAs). We explore its experimental shortcomings, issues with temporally shifted benchmarks, and what this means for machine learning awards.",section:"Posts",handler:()=>{window.location.href="/blog/2024/calibrated-mia/"}},{id:"post-my-submission-to-the-eti-challenge",title:"My submission to the ETI Challenge",description:"Description of my entry to the ETI (Erasing the Invisible) challenge (co-located with NeurIPS) for watermark-removal.",section:"Posts",handler:()=>{window.location.href="/blog/2024/eti-submission/"}},{id:"post-my-submission-to-the-tdc-trojan-detection-challenge",title:"My submission to the TDC Trojan Detection Challenge",description:"Description of my entry to the TDC Trojan Detection challenge (co-located with NeurIPS 2023).",section:"Posts",handler:()=>{window.location.href="/blog/2023/tdc/"}},{id:"post-my-submission-to-the-mico-challenge",title:"My submission to the MICO Challenge",description:"Description of my entry to the MICO challenge (co-located with SaTML) for membership inference that won me the 2nd place on the CIFAR track.",section:"Posts",handler:()=>{window.location.href="/blog/2023/mico/"}},{id:"post-dissecting-distribution-inference",title:"Dissecting Distribution Inference",description:"Describing our work on distribution inference attacks.",section:"Posts",handler:()=>{window.location.href="/blog/2022/ddi/"}},{id:"post-running-scripts-on-rivanna-at-uva",title:"Running scripts on Rivanna at UVA",description:"A tutorial on how to run scripts on Rivanna (SLURM in general) cluster at UVA, along with some tricks.",section:"Posts",handler:()=>{window.location.href="/blog/2022/uva-rivanna/"}},{id:"post-on-the-risks-of-distribution-inference",title:"On the Risks of Distribution Inference",description:"A blog post describing our work on Property Inference attacks.",section:"Posts",handler:()=>{window.location.href="/blog/2021/distr-inf/"}},{id:"post-reassessing-adversarial-training-with-fixed-data-augmentation",title:"Reassessing adversarial training with fixed data augmentation",description:"A recent bug discovery on Pytorch+Numpy got me thinking- how much does this bug impact adversarial robustness?",section:"Posts",handler:()=>{window.location.href="/blog/2021/advrob-aug/"}},{id:"news-passed-my-phd-dissertation-proposal-grin",title:'Passed my PhD Dissertation Proposal <img class="emoji" title=":grin:" alt=":grin:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f601.png" height="20" width="20">',description:"",section:"News"},{id:"news-runner-up-https-microsoft-github-io-mico-for-the-mico-challenge-cifar-track-co-located-with-satml-tada-i-describe-my-approach-here-blog-2023-mico",title:'[Runner-up](https://microsoft.github.io/MICO/) for the MICO challenge (CIFAR track), co-located with SaTML <img class="emoji" title=":tada:" alt=":tada:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f389.png" height="20" width="20"> I describe...',description:"",section:"News"},{id:"news-placard-presented-our-work-on-dissecting-distribution-inference-https-arxiv-org-pdf-2212-07591-at-satml-in-raleigh-us",title:'<img class="emoji" title=":placard:" alt=":placard:" src="https://github.githubassets.com/images/icons/emoji/unicode/1faa7.png" height="20" width="20"> Presented our work on [Dissecting Distribution Inference](https://arxiv.org/pdf/2212.07591) at SaTML in Raleigh! <img class="emoji" title=":us:" alt=":us:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f1fa-1f1f8.png" height="20" width="20">...',description:"",section:"News"},{id:"news-received-the-john-a-stankovic-graduate-research-award-https-engineering-virginia-edu-department-computer-science-blogs-cs-department-end-year-award-recipients-2022-2023-from-the-cs-deparment-at-uva-for-excellence-in-research-during-the-2022-2023-academic-year",title:"Received the [John A. Stankovic Graduate Research Award](https://engineering.virginia.edu/department/computer-science/blogs/cs-department-end-year-award-recipients-2022-2023) from the CS deparment at...",description:"",section:"News"},{id:"news-leading-a-reading-group-books-on-causal-learning-https-iamgroot42-github-io-causal-reading-group-23-this-summer-at-uva-along-with-hannah-chen-https-hannahxchen-github-io",title:'Leading a reading group <img class="emoji" title=":books:" alt=":books:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f4da.png" height="20" width="20"> on [Causal Learning](https://iamgroot42.github.io/causal-reading-group-23/) this summer at UVA, along...',description:"",section:"News"},{id:"news-received-the-endowed-graduate-fellowship-https-www-linkedin-com-posts-activity-7122237223894192128-shmf-utm-source-share-amp-utm-medium-member-desktop-from-the-school-of-engineering-and-applied-sciences-seas-at-uva-for-2023-24-tada",title:"Received the [Endowed Graduate Fellowship](https://www.linkedin.com/posts/activity-7122237223894192128-SHMF?utm_source=share&utm_medium=member_desktop) from the School of Engineering and Applied Sciences...",description:"",section:"News"},{id:"news-placard-presented-our-work-sok-pitfalls-in-evaluating-black-box-attacks-https-arxiv-org-pdf-2310-17534-pdf-at-satml-in-toronto-canada",title:'<img class="emoji" title=":placard:" alt=":placard:" src="https://github.githubassets.com/images/icons/emoji/unicode/1faa7.png" height="20" width="20"> Presented our work [SoK: Pitfalls in Evaluating Black-Box Attacks](https://arxiv.org/pdf/2310.17534.pdf) at SaTML in...',description:"",section:"News"},{id:"news-ph-done-say-hello-to-dr-suri",title:"Ph-Done! Say hello to Dr. Suri",description:"",section:"News",handler:()=>{window.location.href="/news/ph_done/"}},{id:"news-xiao-https-xiao-zhang-net-presented-our-work-do-parameters-reveal-more-than-loss-for-membership-inference-https-arxiv-org-pdf-2406-11544-at-the-hild-workshop-at-icml-https-sites-google-com-view-hidimlearning-home-in-vienna-austria",title:"[Xiao](https://xiao-zhang.net/) presented our work [Do Parameters Reveal More than Loss for Membership Inference](https://arxiv.org/pdf/2406.11544)...",description:"",section:"News"},{id:"news-started-as-a-postdoc-at-khoury-northeastern-supervised-by-alina-oprea-https-www-khoury-northeastern-edu-home-alina-hello-boston-cityscape",title:"Started as a postdoc at Khoury, Northeastern supervised by [Alina Oprea](https://www.khoury.northeastern.edu/home/alina/). Hello, Boston...",description:"",section:"News"},{id:"news-placard-presented-our-work-do-membership-inference-attacks-work-on-large-language-models-https-arxiv-org-pdf-2402-07841-at-colm-https-colmweb-org-cfp-html-in-philadelphia-sunny",title:'<img class="emoji" title=":placard:" alt=":placard:" src="https://github.githubassets.com/images/icons/emoji/unicode/1faa7.png" height="20" width="20"> Presented our work [Do Membership Inference Attacks Work on Large Language Models?](https://arxiv.org/pdf/2402.07841)...',description:"",section:"News"},{id:"news-scroll-my-first-patent-https-patents-google-com-patent-us12130929b2-was-just-issued-this-is-based-on-work-https-arxiv-org-pdf-2206-03317-that-our-team-did-during-my-internship-at-oracle-research-back-in-fall-2021-patents-sure-are-slow",title:'<img class="emoji" title=":scroll:" alt=":scroll:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f4dc.png" height="20" width="20"> My first [patent](https://patents.google.com/patent/US12130929B2/) was just issued! This is based on [work](https://arxiv.org/pdf/2206.03317) that...',description:"",section:"News"},{id:"news-newspaper-uva-engineering-covered-a-story-https-engineering-virginia-edu-news-events-news-common-way-test-leaks-large-language-models-may-be-flawed-on-our-work-on-evaluating-membership-inference-attacks-on-large-language-models-https-arxiv-org-pdf-2402-07841",title:'<img class="emoji" title=":newspaper:" alt=":newspaper:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f4f0.png" height="20" width="20"> UVA Engineering [covered a story](https://engineering.virginia.edu/news-events/news/common-way-test-leaks-large-language-models-may-be-flawed) on our work on evaluating [Membership Inference...',description:"",section:"News"},{id:"news-our-blogpost-blog-2024-calibrated-mia-talking-about-critical-flaws-in-the-evaluation-of-a-recent-emnlp-best-paper-https-x-com-emnlpmeeting-status-1857176180128198695-has-been-accepted-to-the-iclr-blogpost-track",title:"Our [blogpost](blog/2024/calibrated-mia/) talking about critical flaws in the evaluation of a recent [EMNLP...",description:"",section:"News"},{id:"news-awarded-lambda-research-grant-to-work-on-training-time-poisoning-defenses-for-llms-thank-you-lambda-moneybag",title:"Awarded Lambda Research Grant to work on training-time poisoning defenses for LLMs. Thank...",description:"",section:"News"},{id:"projects-project-1",title:"project 1",description:"with background image",section:"Projects",handler:()=>{window.location.href="/projects/1_project/"}},{id:"projects-project-2",title:"project 2",description:"a project with a background image and giscus comments",section:"Projects",handler:()=>{window.location.href="/projects/2_project/"}},{id:"projects-project-3-with-very-long-name",title:"project 3 with very long name",description:"a project that redirects to another website",section:"Projects",handler:()=>{window.location.href="/projects/3_project/"}},{id:"projects-project-4",title:"project 4",description:"another without an image",section:"Projects",handler:()=>{window.location.href="/projects/4_project/"}},{id:"projects-project-5",title:"project 5",description:"a project with a background image",section:"Projects",handler:()=>{window.location.href="/projects/5_project/"}},{id:"projects-project-6",title:"project 6",description:"a project with no image",section:"Projects",handler:()=>{window.location.href="/projects/6_project/"}},{id:"projects-project-7",title:"project 7",description:"with background image",section:"Projects",handler:()=>{window.location.href="/projects/7_project/"}},{id:"projects-project-8",title:"project 8",description:"an other project with a background image and giscus comments",section:"Projects",handler:()=>{window.location.href="/projects/8_project/"}},{id:"projects-project-9",title:"project 9",description:"another project with an image \ud83c\udf89",section:"Projects",handler:()=>{window.location.href="/projects/9_project/"}},{id:"talks-bust-in-silhouette-project-personality-chat-intelligent-cloud-conference",title:'<img class="emoji" title=":bust_in_silhouette:" alt=":bust_in_silhouette:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f464.png" height="20" width="20"> Project Personality Chat. Intelligent Cloud Conference.',description:"",section:"Talks"},{id:"talks-computer-when-models-learn-too-much-inference-privacy-in-theory-and-practice-with-david-evans-microsoft-security-data-science-colloquium",title:'<img class="emoji" title=":computer:" alt=":computer:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f4bb.png" height="20" width="20"> When Models Learn Too Much: Inference Privacy in Theory and Practice (with...',description:"",section:"Talks"},{id:"talks-computer-formalizing-distribution-inference-risks-lockheed-martin",title:'<img class="emoji" title=":computer:" alt=":computer:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f4bb.png" height="20" width="20"> Formalizing Distribution Inference Risks: Lockheed Martin.',description:"",section:"Talks"},{id:"talks-bust-in-silhouette-privacy-in-genomics-https-computingbiology-github-io-s22-class18-cs4501-computational-biology-biological-computing",title:'<img class="emoji" title=":bust_in_silhouette:" alt=":bust_in_silhouette:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f464.png" height="20" width="20"> [Privacy in Genomics](https://computingbiology.github.io/s22/class18/). CS4501: Computational Biology / Biological Computing',description:"",section:"Talks"},{id:"talks-computer-distribution-inference-university-of-melbourne",title:'<img class="emoji" title=":computer:" alt=":computer:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f4bb.png" height="20" width="20"> Distribution Inference. University of Melbourne.',description:"",section:"Talks"},{id:"talks-computer-formalizing-and-estimating-distribution-inference-risks-iiith",title:'<img class="emoji" title=":computer:" alt=":computer:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f4bb.png" height="20" width="20"> Formalizing and Estimating Distribution Inference Risks: IIITH.',description:"",section:"Talks"},{id:"talks-computer-distribution-inference-new-perspectives-in-data-privacy-cs562-advanced-topics-in-security-privacy-and-machine-learning-uiuc-https-chandrasekaran-group-github-io-courses-cs562-home",title:'<img class="emoji" title=":computer:" alt=":computer:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f4bb.png" height="20" width="20"> Distribution Inference: New Perspectives in Data Privacy. [CS562 Advanced Topics in Security,...',description:"",section:"Talks"},{id:"talks-bust-in-silhouette-distribution-inference-new-perspectives-in-data-privacy-princeton-https-ece-princeton-edu-events-distribution-inference-new-perspectives-data-privacy",title:'<img class="emoji" title=":bust_in_silhouette:" alt=":bust_in_silhouette:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f464.png" height="20" width="20"> Distribution Inference: New Perspectives in Data Privacy. [Princeton](https://ece.princeton.edu/events/distribution-inference-new-perspectives-data-privacy).',description:"",section:"Talks"},{id:"talks-bust-in-silhouette-distribution-inference-new-perspectives-in-data-privacy-uva-aiml-seminar-https-uvaml-github-io-pasttalks-2024-02-28",title:'<img class="emoji" title=":bust_in_silhouette:" alt=":bust_in_silhouette:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f464.png" height="20" width="20"> Distribution Inference: New Perspectives in Data Privacy. [UVA AIML Seminar](https://uvaml.github.io/pasttalks/2024-02-28/).',description:"",section:"Talks"},{id:"talks-computer-do-membership-inference-attacks-work-on-large-language-models-https-drive-google-com-file-d-1vkahsahwkmy4psti7f4k0z26gfakxrv2-view-cohere-for-ai",title:'<img class="emoji" title=":computer:" alt=":computer:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f4bb.png" height="20" width="20"> [Do Membership Inference Attacks Work on Large Language Models?](https://drive.google.com/file/d/1vKAHsahwKmy4PsTi7f4K0Z26gfAkXRV2/view) Cohere for AI....',description:"",section:"Talks"},{id:"talks-bust-in-silhouette-do-membership-inference-attacks-work-on-large-language-models-google-research",title:'<img class="emoji" title=":bust_in_silhouette:" alt=":bust_in_silhouette:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f464.png" height="20" width="20"> Do Membership Inference Attacks Work on Large Language Models? Google Research.',description:"",section:"Talks"},{id:"talks-bust-in-silhouette-do-membership-inference-attacks-work-on-large-language-models-university-of-washington",title:'<img class="emoji" title=":bust_in_silhouette:" alt=":bust_in_silhouette:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f464.png" height="20" width="20"> Do Membership Inference Attacks Work on Large Language Models? University of Washington....',description:"",section:"Talks"},{id:"talks-computer-do-membership-inference-attacks-work-on-large-language-models-safr-ai-lab-https-sethneel-com-safr-ai-lab-harvard-university",title:'<img class="emoji" title=":computer:" alt=":computer:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f4bb.png" height="20" width="20"> Do Membership Inference Attacks Work on Large Language Models? [SAFR AI Lab](https://sethneel.com/safr-ai-lab/),...',description:"",section:"Talks"},{id:"talks-computer-do-membership-inference-attacks-work-on-large-language-models-privacy-team-cas-meta-https-research-facebook-com-teams-cas",title:'<img class="emoji" title=":computer:" alt=":computer:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f4bb.png" height="20" width="20"> Do Membership Inference Attacks Work on Large Language Models? [Privacy Team, CAS,...',description:"",section:"Talks"},{id:"talks-computer-distribution-inference-new-perspectives-in-data-privacy-vector-institute",title:'<img class="emoji" title=":computer:" alt=":computer:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f4bb.png" height="20" width="20"> Distribution Inference: New Perspectives in Data Privacy. Vector Institute.',description:"",section:"Talks"},{id:"talks-computer-white-box-v-s-black-box-privacy-auditing-for-machine-learning-ai-security-and-privacy-umass-ahmerst-https-aisec-cs-umass-edu",title:'<img class="emoji" title=":computer:" alt=":computer:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f4bb.png" height="20" width="20"> White-box v/s Black-box: Privacy Auditing for Machine Learning. [AI Security and Privacy,...',description:"",section:"Talks"},{id:"talks-computer-stealthy-membership-inference-for-retrieval-augmented-generation-fair-ml-university-of-south-florida-https-www-anshumanc-com",title:'<img class="emoji" title=":computer:" alt=":computer:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f4bb.png" height="20" width="20"> Stealthy Membership Inference for Retrieval-Augmented Generation. [Fair ML, University of South Florida](https://www.anshumanc.com/)....',description:"",section:"Talks"},{id:"socials-email",title:"Send email",section:"Socials",handler:()=>{window.open("mailto:%61%6E%73.%73%75%72%69@%6E%6F%72%74%68%65%61%73%74%65%72%6E.%65%64%75","_blank")}},{id:"socials-google-scholar",title:"Google Scholar",section:"Socials",handler:()=>{window.open("https://scholar.google.com/citations?user=JDp__3wAAAAJ","_blank")}},{id:"socials-github",title:"GitHub",section:"Socials",handler:()=>{window.open("https://github.com/iamgroot42","_blank")}},{id:"socials-linkedin",title:"LinkedIn",section:"Socials",handler:()=>{window.open("https://www.linkedin.com/in/iamgroot42","_blank")}},{id:"socials-x",title:"X",description:"Twitter",section:"Socials",handler:()=>{window.open("https://twitter.com/iamgroot42","_blank")}},{id:"socials-bluesky",title:"Bluesky",section:"Socials",handler:()=>{window.open("https://bsky.app/profile/iamgroot42.bsky.social","_blank")}},{id:"socials-rss",title:"RSS Feed",section:"Socials",handler:()=>{window.open("/feed.xml","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>